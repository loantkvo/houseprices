{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9e15ad43-8a3a-4840-9c0d-afa2ba2bd148",
    "_uuid": "16485c4c885b6416be79eade3cfff6345861a8bd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "#from pylab import rcParams\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 500)\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "#tf.logging.set_verbosity(tf.logging.INFO)\n",
    "#sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9e15ad43-8a3a-4840-9c0d-afa2ba2bd148",
    "_uuid": "16485c4c885b6416be79eade3cfff6345861a8bd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('train.csv')\n",
    "print('Shape of the train data:', X_train.shape)\n",
    "X_train.drop('Id', axis = 1, inplace=True)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_feature = pd.Index(['SalePrice'])\n",
    "Y_train = X_train[target_feature].astype('float64')\n",
    "X_train.drop(target_feature, axis=1, inplace=True)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9e15ad43-8a3a-4840-9c0d-afa2ba2bd148",
    "_uuid": "16485c4c885b6416be79eade3cfff6345861a8bd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('test.csv')\n",
    "print('Shape of the test data:', X_test.shape)\n",
    "X_test_id = X_test.Id\n",
    "X_test.drop('Id', axis = 1, inplace=True)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(X_train.shape, X_train.isnull().values.sum())\n",
    "X_train_categorical = X_train.select_dtypes(include=np.object).fillna('NONE')\n",
    "X_train_numeric = X_train.select_dtypes(exclude=np.object).fillna(0).astype('float64')\n",
    "X_train = pd.concat([X_train_categorical, X_train_numeric], axis=1)\n",
    "cat_features = X_train_categorical.columns\n",
    "num_features = X_train_numeric.columns\n",
    "#print(X_train.shape, X_train.isnull().values.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train_numeric), X_train_numeric.shape)\n",
    "dummy = X_test[X_train_numeric.columns]\n",
    "print(type(dummy), dummy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(X_test.shape, X_test.isnull().values.sum())\n",
    "X_test_categorical = X_test[X_train_categorical.columns].fillna('NONE')\n",
    "X_test_numeric = X_test[X_train_numeric.columns].fillna(0)\n",
    "X_test = pd.concat([X_test_categorical, X_test_numeric], axis=1)\n",
    "#print(X_test.shape, X_test.isnull().values.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "599c148c-bee5-424e-80cc-afc1d7b1e3fb",
    "_uuid": "08498903317073ce7632c3fbff677ae842ca8389",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_outliers(df_numeric, methods=None, parameters={}):\n",
    "    '''\n",
    "    https://scikit-learn.org/stable/modules/outlier_detection.html#isolation-forest\n",
    "    '''\n",
    "    if methods=='IsolationForest':\n",
    "        from sklearn.ensemble import IsolationForest\n",
    "        clf = IsolationForest(max_samples = parameters.get('max_samples', 'auto'), \n",
    "                              random_state = parameters.get('random_state', 42), \n",
    "                              behaviour = parameters.get('behaviour', 'new'), \n",
    "                              contamination = parameters.get('contamination', 'auto'))\n",
    "        isoforest_mask = clf.fit_predict(df_numeric)\n",
    "        return isoforest_mask==1\n",
    "    elif methods=='normal':\n",
    "        from scipy import stats\n",
    "        #check the axis value here https://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-pandas-data-frame\n",
    "        return (np.abs(stats.zscore(df, axis=1)) < 2).all(axis=0)\n",
    "    else:\n",
    "        return np.array(np.bool(np.ones(df_numeric.shape[0],1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "599c148c-bee5-424e-80cc-afc1d7b1e3fb",
    "_uuid": "08498903317073ce7632c3fbff677ae842ca8389"
   },
   "outputs": [],
   "source": [
    "print(\"Number of samples before removing outliner in train set:\", X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "599c148c-bee5-424e-80cc-afc1d7b1e3fb",
    "_uuid": "08498903317073ce7632c3fbff677ae842ca8389",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "isoforest_mask = remove_outliers(X_train_numeric, 'IsolationForest')\n",
    "X_train = X_train.loc[isoforest_mask]\n",
    "X_train.reset_index(drop = True, inplace = True)#drop=True: avoid the old index being added as a column\n",
    "X_train_numeric = X_train_numeric.loc[isoforest_mask]\n",
    "X_train_numeric.reset_index(drop = True, inplace = True) \n",
    "X_train_categorical = X_train_categorical.loc[isoforest_mask]\n",
    "X_train_categorical.reset_index(drop = True, inplace = True)\n",
    "Y_train = Y_train.loc[isoforest_mask]\n",
    "Y_train.reset_index(drop = True, inplace = True)\n",
    "print(\"Number of samples after removing outliner in train set:\", X_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale (normalize or standardize) BOTH features and TARGET\n",
    "\n",
    "Excellent discussion about data scaling:\n",
    "https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/\n",
    "It concludes that IN NEURON NETWORK REGRESSION we should scale not only the features but also the target \n",
    "\n",
    "\n",
    "The following post also has a nice illustration about the effect of data scaling. However, its conclusion seems WRONG - although it is safe to say \"Normalizing the output will not affect shape of ð‘“\", but large target y might result in large gradient --> parameters are updated with large values --> might explode\n",
    "https://stats.stackexchange.com/questions/111467/is-it-necessary-to-scale-the-target-value-in-addition-to-scaling-features-for-re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0639e24d-202b-461c-b708-0c7d5bc3aa18",
    "_uuid": "b4b2a5534771950433404e618d6b002a913606a2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaling_type = 'MinMaxScaler' # 'StandardScaler'\n",
    "if scaling_type == 'MinMaxScaler':\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    input_scaler = MinMaxScaler()\n",
    "    output_scaler = MinMaxScaler()\n",
    "elif scaling_type == 'StandardScaler':\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    input_scaler = StandardScaler()\n",
    "    output_scaler = StandardScaler()\n",
    "else: #not scale input/output\n",
    "    input_scaler = None\n",
    "    output_scaler = None\n",
    "    \n",
    "if input_scaler is not None:\n",
    "    # fit scaler\n",
    "    input_scaler.fit(X_train_numeric)\n",
    "    # transform training input\n",
    "    X_train_numeric = pd.DataFrame(input_scaler.transform(X_train_numeric), columns=num_features)\n",
    "    # transform test input\n",
    "    X_test_numeric = pd.DataFrame(input_scaler.transform(X_test_numeric), columns=num_features)\n",
    "\n",
    "if output_scaler is not None:\n",
    "    # fit scaler on training output\n",
    "    output_scaler.fit(Y_train)\n",
    "    # transform training output\n",
    "    Y_train = pd.DataFrame(output_scaler.transform(Y_train), columns=target_feature)\n",
    "    # inverse transform: output = output_scaler.inverse_transform(scaled_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "06a63dfe-4b39-4281-a240-7620595dcb87",
    "_uuid": "043062c7d11e3503cf2d60da1e3eb46639228a36"
   },
   "source": [
    "# Set-up the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "268dc739-d13f-42e5-8eb0-f0f16e75b41b",
    "_uuid": "dae02f368a2cc5b5ca248e55e5b63e1e3477993e"
   },
   "outputs": [],
   "source": [
    "tf_features =  [tf.contrib.layers.real_valued_column(numeric_feature) \n",
    "                        for numeric_feature in X_train_numeric.columns]\n",
    "\n",
    "for categorical_feature in X_train_categorical.columns:\n",
    "    temp = tf.contrib.layers.sparse_column_with_hash_bucket(categorical_feature, \n",
    "                                                            hash_bucket_size=1000) \n",
    "    tf_features.append(tf.contrib.layers.embedding_column(sparse_id_column=temp, \n",
    "                                                          dimension=16,combiner=\"sum\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "268dc739-d13f-42e5-8eb0-f0f16e75b41b",
    "_uuid": "dae02f368a2cc5b5ca248e55e5b63e1e3477993e"
   },
   "outputs": [],
   "source": [
    "# split the original training data into train and dev\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(X_train.values, Y_train.values, \n",
    "                                                  test_size=0.33, random_state=42)\n",
    "# train\n",
    "y_train = pd.DataFrame(y_dev, columns = Y_train.columns)\n",
    "xy_train = pd.DataFrame(x_train, columns = X_train.columns).merge(y_train, left_index = True, right_index = True)\n",
    "# dev\n",
    "y_dev = pd.DataFrame(y_dev, columns = Y_train.columns)\n",
    "xy_dev = pd.DataFrame(x_dev, columns = X_train.columns).merge(y_dev, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "268dc739-d13f-42e5-8eb0-f0f16e75b41b",
    "_uuid": "dae02f368a2cc5b5ca248e55e5b63e1e3477993e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Training for submission\n",
    "training_sub = training_set[FEATURES + FEATURES_CAT]\n",
    "testing_sub = test[FEATURES + FEATURES_CAT]\n",
    "\n",
    "training_set[FEATURES_CAT] = training_set[FEATURES_CAT].applymap(str)\n",
    "testing_set[FEATURES_CAT] = testing_set[FEATURES_CAT].applymap(str)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f3df3f0c-9d5a-460b-9913-bd5768c3c665",
    "_uuid": "8069ef0a0ba6e38f4139e3b376e4e4de5546816d"
   },
   "outputs": [],
   "source": [
    "def input_fn_new(data_set, training = True):\n",
    "    \n",
    "    continuous_cols = {k: tf.constant(data_set[k].values) for k in num_features}\n",
    "    \n",
    "    categorical_cols = {k: tf.SparseTensor(\n",
    "        indices = [[i, 0] for i in range(data_set[k].size)], \n",
    "        values = data_set[k].values, \n",
    "        dense_shape = [data_set[k].size, 1]) for k in cat_features}\n",
    "\n",
    "    # Merges the two dictionaries\n",
    "    feature_cols = dict(list(continuous_cols.items()) + list(categorical_cols.items()))\n",
    "    \n",
    "    if training == True:\n",
    "        label = tf.constant(data_set[LABEL].values)\n",
    "        return feature_cols, label\n",
    "    \n",
    "    return feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn and apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f3df3f0c-9d5a-460b-9913-bd5768c3c665",
    "_uuid": "8069ef0a0ba6e38f4139e3b376e4e4de5546816d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "regressor = tf.contrib.learn.DNNRegressor(feature_columns = tf_features, \n",
    "                                          activation_fn = tf.nn.relu, hidden_units=[200, 100, 50, 25, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "874777ec-071a-4e2f-9e60-dcbc72487487",
    "_uuid": "71daf4fea970ed7a9232c67b5664fdaaf276203b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# learn the network given training data\n",
    "regressor.fit(input_fn = lambda: input_fn_new(xy_train) , steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "242a2c81-5736-4322-ae86-8ff474ac1438",
    "_uuid": "343de6e9ea87548d71fb379d5d3455d769b1fdf8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ev = regressor.evaluate(input_fn=lambda: input_fn_new(xy_dev, training = True), steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bc5b50b5-5c58-471d-a4ac-18d5b3021ebf",
    "_uuid": "efa77284a7415bd742573492cc86a37eeff977a2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_dev = ev[\"loss\"]\n",
    "print(\"Loss on the dev set: {0:f}\".format(loss_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3c1a3d70-26b9-4725-a431-09688c61b135",
    "_uuid": "71b34cbf12b26059ea3ddab595ddae87b1e937b4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yhat_testing = regressor.predict(input_fn=lambda: input_fn_new(Y_test))\n",
    "predictions = list(itertools.islice(y, testing_set.shape[0]))\n",
    "predictions = pd.DataFrame(prepro_y.inverse_transform(np.array(predictions).reshape(434,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6e1fa3c8-6bc0-4f44-88bb-e102bbc15bab",
    "_uuid": "f202b4cc9c46c2cec55f144ad457233e2d892cfa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matplotlib.rc('xtick', labelsize=30) \n",
    "matplotlib.rc('ytick', labelsize=30) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(50, 40))\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.plot(predictions.values, reality.values, 'ro')\n",
    "plt.xlabel('Predictions', fontsize = 30)\n",
    "plt.ylabel('Reality', fontsize = 30)\n",
    "plt.title('Predictions x Reality on dataset Test', fontsize = 30)\n",
    "ax.plot([reality.min(), reality.max()], [reality.min(), reality.max()], 'k--', lw=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "784393b4-b463-4245-9256-c14dffa30b12",
    "_uuid": "9f13d3b60f511acbb91056cf1b12d037b60146be",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_predict = regressor.predict(input_fn=lambda: input_fn_new(testing_sub, training = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "24bc5061-452d-4e05-9d13-97cb7aab8289",
    "_uuid": "1c9106270fd38e149054c6fcd1576c67a2db3a48",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "to_submit(y_predict, \"submission_cont_categ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "51ab5586-53f3-43e0-926b-e178f856f5a1",
    "_uuid": "ee22d288772717d0f61fa1ca8db79b8f765ecccc"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9547c21a-4f23-49cf-8cc6-269c328b8ec5",
    "_uuid": "548cadf701cde3e7850171dd6f52a24fd42add0f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_score = [loss_score1, loss_score2, loss_score3, loss_score4,loss_score5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5b1f7fb1-f8ba-4176-9bc6-a7c85d520e50",
    "_uuid": "38ac30fe3da8cd6a6bd98e78c8aaadbfa16ccb7a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "objects = list_model\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = list_score\n",
    " \n",
    "plt.barh(y_pos, performance, align='center', alpha=0.9)\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('Loss ')\n",
    "plt.title('Model compared without hypertuning')\n",
    " \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
